"""
snake-ia.py - Agent IA bas√© sur Deep Q-Learning
S'int√®gre avec le jeu Snake de serpent.py
"""

import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
import numpy as np
import argparse
import sys
import time
import pygame
from serpent import (
    Snake, Apple, GRID_SIZE, UP, DOWN, LEFT, RIGHT,
    SCREEN_WIDTH, SCREEN_HEIGHT, SCORE_PANEL_HEIGHT,
    draw_grid, display_info, display_message,
    GRIS_FOND, NOIR
)

# ============================================================================
# PARTIE 1: DEEP Q-LEARNING
# ============================================================================

class Linear_QNet(nn.Module):
    """
    R√©seau neuronal pour l'approximation de la fonction Q.
    Input: vecteur d'√©tat am√©lior√© avec distances normalis√©es
    Output: 4 Q-valeurs (une par direction)
    """
    def __init__(self, input_size=15, hidden_size=512, output_size=4):
        super(Linear_QNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.dropout1 = nn.Dropout(0.2)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.dropout2 = nn.Dropout(0.2)
        self.fc3 = nn.Linear(hidden_size, 256)
        self.fc4 = nn.Linear(256, output_size)
        
    def forward(self, x):
        """Forward pass du r√©seau avec dropout pour √©viter l'overfitting."""
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x


class QTrainer:
    """
    Entra√Æneur pour le mod√®le Q-Learning avec PyTorch.
    Impl√©mente la boucle d'entra√Ænement et les mises √† jour du r√©seau.
    """
    def __init__(self, model, lr=0.001, gamma=0.9):
        """
        Args:
            model: R√©seau neuronal Linear_QNet
            lr: Learning rate
            gamma: Facteur de discount
        """
        self.model = model
        self.lr = lr
        self.gamma = gamma
        self.optimizer = optim.Adam(model.parameters(), lr=lr)
        self.criterion = nn.MSELoss()
        
    def train_step(self, state, action, reward, next_state, game_over):
        """
        Effectue une √©tape d'entra√Ænement sur une transition.
        
        Args:
            state: √âtat actuel (tenseur)
            action: Action prise (index 0-3)
            reward: R√©compense re√ßue
            next_state: √âtat suivant (tenseur)
            game_over: Bool√©en indiquant si le jeu est termin√©
        """
        # Conversion en tenseurs si n√©cessaire
        if not isinstance(state, torch.Tensor):
            state = torch.tensor(state, dtype=torch.float32)
        if not isinstance(next_state, torch.Tensor):
            next_state = torch.tensor(next_state, dtype=torch.float32)
        
        # Pr√©dictions Q pour l'√©tat actuel
        prediction = self.model(state)
        target = prediction.clone()
        
        # Calcul de la Q-valeur cible
        if game_over:
            q_new = reward
        else:
            q_new = reward + self.gamma * torch.max(self.model(next_state))
        
        # Mise √† jour de la Q-valeur pour l'action prise
        target[action] = q_new
        
        # Calcul de la perte et optimisation
        loss = self.criterion(prediction, target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


class Agent:
    """
    Agent d'apprentissage par renforcement utilisant Deep Q-Learning.
    G√®re la m√©moire, l'exploration et l'exploitation, l'entra√Ænement.
    """
    def __init__(self, gamma=0.9, epsilon=1.0, epsilon_decay=0.995, 
                 epsilon_min=0.01, learning_rate=0.001, batch_size=32, max_memory=100000):
        """
        Initialisation de l'agent IA.
        
        Args:
            gamma: Facteur de discount
            epsilon: Param√®tre d'exploration (epsilon-greedy)
            epsilon_decay: Taux de d√©croissance de epsilon
            epsilon_min: Valeur minimale de epsilon
            learning_rate: Taux d'apprentissage
            batch_size: Taille des batches d'entra√Ænement
            max_memory: Taille maximale de la m√©moire de replay
        """
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.max_memory = max_memory
        
        # Mod√®le et entra√Æneur
        self.model = Linear_QNet()
        self.trainer = QTrainer(self.model, lr=learning_rate, gamma=gamma)
        
        # M√©moire de replay (state, action, reward, next_state, game_over)
        self.memory = deque(maxlen=max_memory)
        
    def get_state(self, snake, apple):
        """
        Construit un vecteur d'√©tat enrichi pour ignorer le corps et se concentrer sur la pomme.
        
        √âtat: 15 caract√©ristiques
        - 3 dangers (face, droite, gauche)
        - 4 directions actuelles
        - 4 positions relatives de la pomme
        - Distance normalis√©e √† la pomme
        - Taille du serpent (normalis√©e)
        
        Args:
            snake: Objet Snake
            apple: Objet Apple
            
        Returns:
            np.array: Vecteur d'√©tat de 15 √©l√©ments
        """
        head_x, head_y = snake.head_pos
        apple_x, apple_y = apple.position
        direction = snake.direction
        
        # Fonction pour v√©rifier les collisions (strictement les murs et l'auto-morsure)
        def is_collision(x, y):
            if x < 0 or x >= GRID_SIZE or y < 0 or y >= GRID_SIZE:
                return True
            if [x, y] in snake.body[1:]:
                return True
            return False
        
        # D√©tection des dangers dans chaque direction
        danger_up = is_collision(head_x + UP[0], head_y + UP[1])
        danger_down = is_collision(head_x + DOWN[0], head_y + DOWN[1])
        danger_left = is_collision(head_x + LEFT[0], head_y + LEFT[1])
        danger_right = is_collision(head_x + RIGHT[0], head_y + RIGHT[1])
        
        # Danger face (selon la direction actuelle)
        if direction == UP:
            danger_face = danger_up
            danger_right_side = danger_right
            danger_left_side = danger_left
        elif direction == DOWN:
            danger_face = danger_down
            danger_right_side = danger_left
            danger_left_side = danger_right
        elif direction == LEFT:
            danger_face = danger_left
            danger_right_side = danger_up
            danger_left_side = danger_down
        else:  # RIGHT
            danger_face = danger_right
            danger_right_side = danger_down
            danger_left_side = danger_up
        
        # Calcul de la distance √† la pomme (distance de Manhattan)
        manhattan_distance = abs(apple_x - head_x) + abs(apple_y - head_y)
        # Normalisation entre 0 et 1
        max_distance = 2 * GRID_SIZE
        normalized_distance = manhattan_distance / max_distance
        
        # Taille du serpent normalis√©e
        snake_size_normalized = len(snake.body) / (GRID_SIZE * GRID_SIZE)
        
        # Construction du vecteur d'√©tat am√©lior√©
        state = np.array([
            danger_face,
            danger_right_side,
            danger_left_side,
            direction == UP,
            direction == DOWN,
            direction == LEFT,
            direction == RIGHT,
            apple_x < head_x,
            apple_x > head_x,
            apple_y < head_y,
            apple_y > head_y,
            normalized_distance,          # Distance √† la pomme
            snake_size_normalized,         # Taille du serpent
            1.0 if manhattan_distance > 1 else 0.0,  # Est loin de la pomme
            1.0 if [apple_x, apple_y] in snake.body else 0.0,  # Pomme accessible
        ], dtype=np.float32)
        
        return state
    
    def remember(self, state, action, reward, next_state, game_over):
        """Stocke une transition dans la m√©moire de replay."""
        self.memory.append((state, action, reward, next_state, game_over))
    
    def train_short_memory(self, state, action, reward, next_state, game_over):
        """Entra√Æne le mod√®le sur une seule transition."""
        state_tensor = torch.tensor(state, dtype=torch.float32)
        next_state_tensor = torch.tensor(next_state, dtype=torch.float32)
        self.trainer.train_step(state_tensor, action, reward, next_state_tensor, game_over)
    
    def train_long_memory(self):
        """Entra√Æne le mod√®le sur un batch al√©atoire de la m√©moire."""
        if len(self.memory) < self.batch_size:
            mini_sample = list(self.memory)
        else:
            mini_sample = random.sample(list(self.memory), self.batch_size)
        
        # Conversion efficace en numpy d'abord, puis en tenseur (beaucoup plus rapide)
        states = np.array([sample[0] for sample in mini_sample], dtype=np.float32)
        actions = np.array([sample[1] for sample in mini_sample], dtype=np.int64)
        rewards = np.array([sample[2] for sample in mini_sample], dtype=np.float32)
        next_states = np.array([sample[3] for sample in mini_sample], dtype=np.float32)
        game_overs = np.array([sample[4] for sample in mini_sample], dtype=bool)
        
        # Conversion en tenseurs PyTorch (une seule fois, tr√®s rapide)
        states_tensor = torch.from_numpy(states)
        next_states_tensor = torch.from_numpy(next_states)
        rewards_tensor = torch.from_numpy(rewards)
        
        # Pr√©dictions pour les √©tats actuels
        predictions = self.model(states_tensor)
        targets = predictions.clone()
        
        for idx in range(len(mini_sample)):
            if game_overs[idx]:
                q_new = rewards_tensor[idx]
            else:
                q_new = rewards_tensor[idx] + self.gamma * torch.max(self.model(next_states_tensor[idx:idx+1]))
            targets[idx, actions[idx]] = q_new
        
        # Entra√Ænement
        loss = self.trainer.criterion(predictions, targets)
        self.trainer.optimizer.zero_grad()
        loss.backward()
        self.trainer.optimizer.step()
        
        # D√©croissance de epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def get_action(self, state, valid_actions=None):
        """Choisit une action selon la strat√©gie epsilon-greedy."""
        if valid_actions is None:
            valid_actions = [0, 1, 2, 3]
        
        # Exploration (al√©atoire)
        if random.random() < self.epsilon:
            return random.choice(valid_actions)
        
        # Exploitation (meilleure action connue)
        state_tensor = torch.tensor(state, dtype=torch.float32)
        with torch.no_grad():
            predictions = self.model(state_tensor)
        
        # Filtre les actions non valides
        for i in range(len(predictions)):
            if i not in valid_actions:
                predictions[i] = float('-inf')
        
        return torch.argmax(predictions).item()
    
    def save_model(self, path="snake_model.pth"):
        """Sauvegarde le mod√®le sur disque."""
        torch.save(self.model.state_dict(), path)
    
    def load_model(self, path="snake_model.pth"):
        """Charge le mod√®le depuis disque."""
        self.model.load_state_dict(torch.load(path))


# ============================================================================
# PARTIE 2: BOUCLE PRINCIPALE ET INT√âGRATION
# ============================================================================

def get_valid_actions(snake):
    """Retourne les actions valides (sans inversion de direction)."""
    current_dir = snake.direction
    valid = []
    
    if current_dir != DOWN:
        valid.append(0)  # UP
    if current_dir != UP:
        valid.append(1)  # DOWN
    if current_dir != RIGHT:
        valid.append(2)  # LEFT
    if current_dir != LEFT:
        valid.append(3)  # RIGHT
    
    return valid


def train_dqn(episodes=1000, render=True):
    """Entra√Æne l'agent Deep Q-Learning avec des r√©compenses am√©lior√©es."""
    pygame.init()
    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))
    pygame.display.set_caption("Snake IA - Entra√Ænement DQN")
    clock = pygame.time.Clock()
    font_main = pygame.font.Font(None, 40)
    
    agent = Agent(learning_rate=0.0005, gamma=0.95, epsilon_decay=0.9995)
    
    print(f"Entra√Ænement DQN pour {episodes} √©pisodes...")
    
    start_time = time.time()
    
    for episode in range(episodes):
        snake = Snake()
        apple = Apple(snake.body)
        state_old = agent.get_state(snake, apple)
        
        total_reward = 0
        steps = 0
        max_steps = 1000
        move_counter = 0
        game_over = False
        prev_distance = abs(apple.position[0] - snake.head_pos[0]) + abs(apple.position[1] - snake.head_pos[1])
        
        while not game_over and steps < max_steps:
            # V√©rifie les √©v√©nements pygame
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    return
            
            # Choix d'une action valide
            valid_actions = get_valid_actions(snake)
            action = agent.get_action(state_old, valid_actions)
            
            # Ex√©cution de l'action
            action_map = [UP, DOWN, LEFT, RIGHT]
            snake.set_direction(action_map[action])
            snake.move()
            
            reward = 0
            
            # V√©rification des collisions
            if snake.is_game_over():
                reward = -10
                game_over = True
            # V√©rification de la pomme mang√©e
            elif snake.head_pos == list(apple.position):
                snake.grow()
                reward = 50  # Grosse r√©compense pour la pomme
                if not apple.relocate(snake.body):
                    game_over = True
            else:
                # R√©compenses interm√©diaires : encourager √† se rapprocher de la pomme
                current_distance = abs(apple.position[0] - snake.head_pos[0]) + abs(apple.position[1] - snake.head_pos[1])
                
                if current_distance < prev_distance:
                    reward = 1  # Petit bonus pour se rapprocher
                elif current_distance > prev_distance:
                    reward = -0.5  # Petite p√©nalit√© pour s'√©loigner
                else:
                    reward = -0.1  # L√©g√®re p√©nalit√© pour ne rien faire
                
                prev_distance = current_distance
            
            total_reward += reward
            
            # Nouvel √©tat
            state_new = agent.get_state(snake, apple)
            
            # Entra√Ænement court terme
            agent.train_short_memory(state_old, action, reward, state_new, game_over)
            
            # M√©morisation
            agent.remember(state_old, action, reward, state_new, game_over)
            
            state_old = state_new
            steps += 1
            
            # Affichage occasionnel
            if render and episode % 50 == 0:
                move_counter += 1
                if move_counter >= 5:
                    # Dessiner
                    screen.fill(GRIS_FOND)
                    game_area_rect = pygame.Rect(0, SCORE_PANEL_HEIGHT, SCREEN_WIDTH, SCREEN_WIDTH)
                    pygame.draw.rect(screen, NOIR, game_area_rect)
                    draw_grid(screen)
                    apple.draw(screen)
                    snake.draw(screen)
                    display_info(screen, font_main, snake, time.time())
                    pygame.display.flip()
                    move_counter = 0
        
        # Entra√Ænement long terme
        agent.train_long_memory()
        
        if (episode + 1) % 50 == 0:
            print(f"√âpisode {episode + 1}/{episodes} - Score: {snake.score} - "
                  f"Reward: {total_reward:.2f} - Epsilon: {agent.epsilon:.4f}")
    
    # Sauvegarde du mod√®le
    agent.save_model("snake_model.pth")
    print("‚úì Mod√®le sauvegard√© dans 'snake_model.pth'")
    elapsed_time = time.time() - start_time
    minutes = int(elapsed_time // 60)
    seconds = elapsed_time % 60
    print(f"üìä Entra√Ænement termin√© en {minutes}m {seconds:.1f}s")
    pygame.quit()


def play_dqn(episodes=10, render=True, model_path="snake_model.pth"):
    """Joue avec le mod√®le DQN entra√Æn√©."""
    pygame.init()
    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))
    pygame.display.set_caption("Snake IA - Mode Jeu DQN")
    clock = pygame.time.Clock()
    font_main = pygame.font.Font(None, 40)
    
    agent = Agent(epsilon=0.0)  # Pas d'exploration en mode jeu
    try:
        agent.load_model(model_path)
        print(f"‚úì Mod√®le charg√© depuis '{model_path}'")
    except FileNotFoundError:
        print(f"‚ö†Ô∏è  Mod√®le non trouv√©: {model_path}")
        print("üí° Entra√Æner d'abord: python3 snake-ia.py --train")
        pygame.quit()
        return
    
    print(f"Jeu avec DQN pour {episodes} parties...")
    
    global_start_time = time.time()
    
    for episode in range(episodes):
        snake = Snake()
        apple = Apple(snake.body)
        state = agent.get_state(snake, apple)
        
        total_reward = 0
        steps = 0
        max_steps = 1000
        move_counter = 0
        game_over = False
        
        start_time = time.time()
        
        while not game_over and steps < max_steps:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    return
            
            valid_actions = get_valid_actions(snake)
            action = agent.get_action(state, valid_actions)
            
            action_map = [UP, DOWN, LEFT, RIGHT]
            snake.set_direction(action_map[action])
            snake.move()
            
            reward = 0
            if snake.is_game_over():
                reward = -10
                game_over = True
            elif snake.head_pos == list(apple.position):
                snake.grow()
                reward = 10
                if not apple.relocate(snake.body):
                    game_over = True
            
            total_reward += reward
            state = agent.get_state(snake, apple)
            steps += 1
            
            if render:
                move_counter += 1
                if move_counter >= 2:
                    screen.fill(GRIS_FOND)
                    game_area_rect = pygame.Rect(0, SCORE_PANEL_HEIGHT, SCREEN_WIDTH, SCREEN_WIDTH)
                    pygame.draw.rect(screen, NOIR, game_area_rect)
                    draw_grid(screen)
                    apple.draw(screen)
                    snake.draw(screen)
                    display_info(screen, font_main, snake, start_time)
                    pygame.display.flip()
                    clock.tick(10)
                    move_counter = 0
        
        print(f"Partie {episode + 1}/{episodes} - Score: {snake.score} - Steps: {steps}")
    
    elapsed_time = time.time() - global_start_time
    minutes = int(elapsed_time // 60)
    seconds = elapsed_time % 60
    print(f"üéÆ Jeu termin√© en {minutes}m {seconds:.1f}s")
    pygame.quit()


def main():
    """Fonction principale avec comportement intelligent."""
    import os
    
    parser = argparse.ArgumentParser(
        description='Snake IA - Deep Q-Learning',
        usage='python3 snake-ia.py [--train EPISODES] [--play EPISODES] [--no-render]'
    )
    
    # D√©terminer le comportement par d√©faut
    model_exists = os.path.exists("snake_model.pth")
    
    parser.add_argument('--train', type=int, default=None, metavar='N',
                        help='Entra√Æner pour N √©pisodes (d√©faut: 1000 si pas de mod√®le)')
    parser.add_argument('--play', type=int, default=None, metavar='N',
                        help='Jouer N parties (d√©faut: 10 si mod√®le existe)')
    parser.add_argument('--no-render', action='store_true',
                        help='D√©sactiver l\'affichage')
    
    args = parser.parse_args()
    
    # Comportement intelligent
    train_mode = args.train is not None
    play_mode = args.play is not None
    
    # Si aucun mode sp√©cifi√©: entra√Æner si pas de mod√®le, sinon jouer
    if not train_mode and not play_mode:
        if model_exists:
            play_mode = True
            args.play = 10
            print("üéÆ Mode jeu (mod√®le trouv√©)")
        else:
            train_mode = True
            args.train = 1000
            print("ü§ñ Mode entra√Ænement (mod√®le absent)")
    
    render = not args.no_render
    
    if train_mode:
        episodes = args.train or 1000
        print(f"üìö Entra√Ænement pendant {episodes} √©pisodes...")
        train_dqn(episodes=episodes, render=render)
    
    if play_mode:
        episodes = args.play or 10
        print(f"üéÆ Jeu pendant {episodes} parties...")
        play_dqn(episodes=episodes, render=render)


if __name__ == '__main__':
    main()

"""
Snake IA - Deep Q-Learning
Impl√©mentation d'un agent d'apprentissage par renforcement pour jouer √† Snake
Utilise PyTorch pour le r√©seau neuronal et le GPU si disponible
"""

import pygame
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import sys
import os
import matplotlib.pyplot as plt
from IPython import display
import torch.cuda.amp as amp  # Mixed precision training pour GPU

# Ajouter le r√©pertoire parent au path pour importer serpent.py
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from serpent import Snake, Apple, GRID_SIZE, CELL_SIZE, SCREEN_WIDTH, SCORE_PANEL_HEIGHT, SCREEN_HEIGHT
from serpent import BLANC, NOIR, ORANGE, VERT, ROUGE, GRIS_FOND, GRIS_GRILLE
from serpent import UP, DOWN, LEFT, RIGHT

# =============================================================================
# HYPERPARAM√àTRES
# =============================================================================
MAX_MEMORY = 100_000  # Taille maximale de la m√©moire de replay
BATCH_SIZE = 3000     # Taille des batches pour l'entra√Ænement
LR = 0.001            # Taux d'apprentissage
GAMMA = 0.8           # Facteur de discount
EPSILON_START = 80    # Exploration initiale
GAME_SPEED = 1000      # Vitesse du jeu pour l'IA

# =============================================================================
# BLOC 1: MODEL (TORCH) - R√©seau de neurones profond
# =============================================================================

class Linear_QNet(nn.Module):
    """
    R√©seau de neurones profond pour le Q-Learning.
    Architecture: Input -> Hidden1 -> Hidden2 -> Output
    """
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, hidden_size)
        self.linear3 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        """Propagation avant √† travers le r√©seau."""
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        x = self.linear3(x)
        return x

    def save(self, file_name='model.pth'):
        """Sauvegarde le mod√®le."""
        model_folder_path = '../groupe13/model'
        if not os.path.exists(model_folder_path):
            os.makedirs(model_folder_path)
        file_name = os.path.join(model_folder_path, file_name)
        torch.save(self.state_dict(), file_name)


class QTrainer:
    """Gestionnaire d'entra√Ænement du mod√®le avec support GPU."""
    def __init__(self, model, lr, gamma):
        self.lr = lr
        self.gamma = gamma
        self.model = model
        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)
        self.criterion = nn.MSELoss()

        # D√©tection automatique du GPU
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        print(f"üñ•Ô∏è  Entra√Ænement sur: {self.device}")
        if torch.cuda.is_available():
            print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")

    def train_step(self, state, action, reward, next_state, done):
        """Entra√Æne le mod√®le sur un batch d'exp√©riences."""
        state = torch.tensor(state, dtype=torch.float).to(self.device)
        next_state = torch.tensor(next_state, dtype=torch.float).to(self.device)
        action = torch.tensor(action, dtype=torch.long).to(self.device)
        reward = torch.tensor(reward, dtype=torch.float).to(self.device)

        if len(state.shape) == 1:
            # Reshape pour un seul √©chantillon (1, x)
            state = torch.unsqueeze(state, 0)
            next_state = torch.unsqueeze(next_state, 0)
            action = torch.unsqueeze(action, 0)
            reward = torch.unsqueeze(reward, 0)
            done = (done, )

        # 1. Pr√©diction Q avec l'√©tat actuel
        pred = self.model(state)

        target = pred.clone()
        for idx in range(len(done)):
            Q_new = reward[idx]
            if not done[idx]:
                # Formule Q-Learning: Q_new = r + Œ≥ * max(Q_next)
                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))

            target[idx][torch.argmax(action[idx]).item()] = Q_new

        # 2. Backpropagation
        self.optimizer.zero_grad()
        loss = self.criterion(target, pred)
        loss.backward()
        self.optimizer.step()


class OptimizedQTrainer(QTrainer):
    """
    Version optimis√©e du QTrainer avec:
    - Mixed precision training (float16) pour GPU - gain ~30%
    - Batch processing vectoris√©
    - Calculs parall√®les sur GPU
    """
    def __init__(self, model, lr, gamma):
        super().__init__(model, lr, gamma)

        # Mixed precision scaler pour GPU (entra√Ænement en float16)
        self.scaler = amp.GradScaler() if torch.cuda.is_available() else None

        if self.scaler:
            print(f"‚ö° Mixed Precision (float16): ACTIV√â - Gain ~30%")

    def train_step_optimized(self, states, actions, rewards, next_states, dones):
        """
        Version optimis√©e de train_step avec mixed precision et vectorisation.
        Traite tout le batch en parall√®le sur le GPU.

        Args:
            states: Liste ou array d'√©tats
            actions: Liste ou array d'actions
            rewards: Liste ou array de r√©compenses
            next_states: Liste ou array de nouveaux √©tats
            dones: Liste ou array de bool√©ens (jeu termin√©)
        """
        # Conversion en tensors sur GPU en une seule fois (tr√®s rapide)
        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)
        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)
        actions = torch.tensor(np.array(actions), dtype=torch.long).to(self.device)
        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(self.device)
        dones = torch.tensor(np.array(dones), dtype=torch.bool).to(self.device)

        if self.scaler:
            # === MODE GPU OPTIMIS√â AVEC MIXED PRECISION ===
            with torch.amp.autocast('cuda'):  # Utilise float16 automatiquement
                # Pr√©diction Q avec l'√©tat actuel (parall√®le sur tout le batch)
                pred = self.model(states)

                # Calcul de Q_target de mani√®re vectoris√©e (TOUT EN PARALL√àLE)
                with torch.no_grad():  # Pas de gradient pour next_states (√©conomie m√©moire)
                    next_q_values = self.model(next_states)
                    max_next_q = torch.max(next_q_values, dim=1)[0]

                    # Q_new = reward + gamma * max(Q_next) si pas done
                    # Op√©ration vectoris√©e : tout le batch en une seule ligne !
                    target_q = rewards + self.gamma * max_next_q * (~dones).float()

                # Mise √† jour des targets (vectoris√©)
                target = pred.clone()
                batch_indices = torch.arange(len(dones)).to(self.device)
                action_indices = torch.argmax(actions, dim=1)
                # Convertir target_q au m√™me dtype que target (float16 en autocast)
                target[batch_indices, action_indices] = target_q.to(target.dtype)

                # Loss sur tout le batch
                loss = self.criterion(pred, target)

            # Backpropagation avec mixed precision (g√®re automatiquement float16/32)
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.optimizer.zero_grad()
        else:
            # === MODE CPU (SANS MIXED PRECISION) ===
            pred = self.model(states)

            with torch.no_grad():
                next_q_values = self.model(next_states)
                max_next_q = torch.max(next_q_values, dim=1)[0]
                target_q = rewards + self.gamma * max_next_q * (~dones).float()

            target = pred.clone()
            batch_indices = torch.arange(len(dones))
            action_indices = torch.argmax(actions, dim=1)
            target[batch_indices, action_indices] = target_q

            loss = self.criterion(pred, target)
            loss.backward()
            self.optimizer.step()
            self.optimizer.zero_grad()

# =============================================================================
# BLOC 2: GAME (PyGame) - Adaptation du jeu pour l'IA
# =============================================================================

class SnakeGameAI:
    """Adaptation du jeu Snake pour l'entra√Ænement IA."""

    def __init__(self, speed=GAME_SPEED, display_enabled=False):
        pygame.init()
        self.speed = speed
        self.display_enabled = display_enabled

        if self.display_enabled:
            # Mode avec affichage visuel
            self.display = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))
            pygame.display.set_caption('Snake IA - Deep Q-Learning')
            self.font = pygame.font.Font(None, 25)
        else:
            # Mode sans affichage (plus rapide)
            # Cr√©e une surface invisible pour que les m√©thodes draw() fonctionnent
            self.display = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT))
            self.font = pygame.font.Font(None, 25)

        self.clock = pygame.time.Clock()
        self.reset()

    def reset(self):
        """R√©initialise le jeu."""
        self.snake = Snake()
        self.apple = Apple(self.snake.body)
        self.frame_iteration = 0
        self.score = 0

    def play_step(self, action):
        """
        Effectue un pas de jeu avec l'action donn√©e.
        Action: [tout droit, tourner droite, tourner gauche]

        Returns:
            reward: r√©compense pour cette action
            game_over: bool√©en indiquant si le jeu est termin√©
            score: score actuel
        """
        self.frame_iteration += 1

        # 1. G√©rer les √©v√©nements pygame
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        # 2. D√©placer le serpent selon l'action
        self._move(action)
        self.snake.move()

        # 3. Calculer la r√©compense
        reward = 0
        game_over = False

        # Timeout si le serpent tourne en rond (√©vite les boucles infinies)
        if self.snake.is_game_over() or self.frame_iteration > 100 * len(self.snake.body):
            game_over = True
            reward = -10  # R√©compense: Perdre -10
            return reward, game_over, self.score

        # 4. V√©rifier si la pomme est mang√©e
        if self.snake.head_pos == list(self.apple.position):
            self.score += 1
            reward = 10  # R√©compense: Attraper la pomme +10
            self.snake.grow()

            # V√©rifier la victoire (toute la grille remplie)
            if not self.apple.relocate(self.snake.body):
                reward = 100  # R√©compense: Finir le jeu +100
                game_over = True

        # 5. Mettre √† jour l'interface (seulement si affichage activ√©)
        if self.display_enabled:
            self._update_ui()
            self.clock.tick(self.speed)

        return reward, game_over, self.score

    def _move(self, action):
        """
        Convertit l'action [tout droit, droite, gauche] en direction.
        Action[0] = tout droit
        Action[1] = tourner √† droite
        Action[2] = tourner √† gauche
        """
        # Directions dans le sens horaire: RIGHT, DOWN, LEFT, UP
        clock_wise = [RIGHT, DOWN, LEFT, UP]
        idx = clock_wise.index(self.snake.direction)

        if np.array_equal(action, [1, 0, 0]):
            # Tout droit - pas de changement
            new_dir = clock_wise[idx]
        elif np.array_equal(action, [0, 1, 0]):
            # Tourner √† droite (sens horaire)
            next_idx = (idx + 1) % 4
            new_dir = clock_wise[next_idx]
        else:  # [0, 0, 1]
            # Tourner √† gauche (sens anti-horaire)
            next_idx = (idx - 1) % 4
            new_dir = clock_wise[next_idx]

        self.snake.set_direction(new_dir)

    def is_collision(self, pt=None):
        """V√©rifie s'il y a collision √† un point donn√©."""
        if pt is None:
            pt = self.snake.head_pos

        # Collision avec les bords
        if pt[0] < 0 or pt[0] >= GRID_SIZE or pt[1] < 0 or pt[1] >= GRID_SIZE:
            return True

        # Collision avec soi-m√™me
        if pt in self.snake.body[1:]:
            return True

        return False

    def _update_ui(self):
        """
        Met √† jour l'affichage du jeu pygame - appel√©e √† chaque frame.

        Cette fonction est responsable de dessiner tous les √©l√©ments visuels:
        1. Le fond et la zone de jeu
        2. La grille de rep√©rage
        3. La pomme (nourriture)
        4. Le serpent (t√™te et corps)
        5. Les informations (score, frames)
        6. Rafra√Æchir l'√©cran

        L'ordre de dessin est important: les √©l√©ments dessin√©s en dernier
        apparaissent au-dessus des pr√©c√©dents (syst√®me de layers).
        """
        # === √âTAPE 1: FOND ===
        # Remplit tout l'√©cran avec la couleur de fond grise
        # Cela efface le contenu de la frame pr√©c√©dente
        self.display.fill(GRIS_FOND)

        # === √âTAPE 2: ZONE DE JEU ===
        # Cr√©e un rectangle noir pour la zone de jeu principale
        # Position: x=0, y=SCORE_PANEL_HEIGHT (d√©cal√© vers le bas pour laisser place au panneau de score)
        # Taille: SCREEN_WIDTH x SCREEN_WIDTH (zone carr√©e)
        game_area_rect = pygame.Rect(0, SCORE_PANEL_HEIGHT, SCREEN_WIDTH, SCREEN_WIDTH)
        # Dessine ce rectangle en noir sur l'√©cran
        pygame.draw.rect(self.display, NOIR, game_area_rect)

        # === √âTAPE 3: GRILLE ===
        # Dessine les lignes de la grille pour visualiser les cellules
        # Facilite la compr√©hension des mouvements du serpent
        self._draw_grid()

        # === √âTAPE 4: POMME ===
        # Dessine la pomme (nourriture) √† sa position actuelle
        # Utilise la m√©thode draw() de l'objet Apple (d√©finie dans serpent.py)
        self.apple.draw(self.display)

        # === √âTAPE 5: SERPENT ===
        # Dessine le serpent complet (t√™te en orange, corps en vert)
        # Utilise la m√©thode draw() de l'objet Snake (d√©finie dans serpent.py)
        # Le serpent est dessin√© apr√®s la pomme pour appara√Ætre au-dessus
        self.snake.draw(self.display)

        # === √âTAPE 6: INFORMATIONS (HUD - Head-Up Display) ===
        # Affiche le score actuel en haut √† gauche
        # font.render() cr√©e une surface de texte avec:
        #   - Le texte √† afficher
        #   - True pour l'antialiasing (lissage)
        #   - BLANC pour la couleur
        score_text = self.font.render(f"Score: {self.score}", True, BLANC)
        # blit() copie la surface de texte sur l'√©cran √† la position (10, 20)
        self.display.blit(score_text, (10, 20))

        # Affiche le nombre de frames (it√©rations) en haut √† droite
        # Utile pour le d√©bogage: permet de voir si l'IA tourne en rond
        # Le timeout est d√©clench√© √† 100 * len(snake.body) frames
        frame_text = self.font.render(f"Frames: {self.frame_iteration}", True, BLANC)
        # Positionn√© √† droite (SCREEN_WIDTH - 150 pixels du bord droit)
        self.display.blit(frame_text, (SCREEN_WIDTH - 150, 20))

        # === √âTAPE 7: RAFRA√éCHISSEMENT ===
        # pygame.display.flip() actualise l'affichage complet
        # Utilise la technique du "double buffering":
        #   - On dessine sur un buffer invisible
        #   - flip() √©change le buffer visible avec l'invisible
        # Cela √©vite le scintillement et assure un affichage fluide
        pygame.display.flip()

    def _draw_grid(self):
        """
        Dessine la grille de rep√©rage sur la zone de jeu.

        Cette fonction trace des lignes verticales et horizontales pour visualiser
        les cellules de la grille. Cela aide √†:
        - Comprendre les mouvements du serpent (cellule par cellule)
        - D√©boguer le positionnement des √©l√©ments
        - Am√©liorer l'aspect visuel du jeu

        La grille utilise la couleur GRIS_GRILLE pour rester discr√®te.
        """
        # === LIGNES VERTICALES ===
        # Parcourt l'√©cran horizontalement par pas de CELL_SIZE (30 pixels)
        # range(0, SCREEN_WIDTH, CELL_SIZE) g√©n√®re: 0, 30, 60, 90, ...
        for x in range(0, SCREEN_WIDTH, CELL_SIZE):
            # pygame.draw.line(surface, couleur, point_d√©part, point_arriv√©e)
            # Ligne du haut (d√©cal√©e par SCORE_PANEL_HEIGHT) au bas de l'√©cran
            pygame.draw.line(self.display, GRIS_GRILLE,
                           (x, SCORE_PANEL_HEIGHT),      # Point de d√©part (x, top)
                           (x, SCREEN_HEIGHT))           # Point d'arriv√©e (x, bottom)

        # === LIGNES HORIZONTALES ===
        # Parcourt l'√©cran verticalement par pas de CELL_SIZE
        # Commence √† SCORE_PANEL_HEIGHT pour ne pas dessiner sur le panneau de score
        for y in range(SCORE_PANEL_HEIGHT, SCREEN_HEIGHT, CELL_SIZE):
            # Ligne de gauche (x=0) √† droite (x=SCREEN_WIDTH)
            pygame.draw.line(self.display, GRIS_GRILLE,
                           (0, y),                       # Point de d√©part (left, y)
                           (SCREEN_WIDTH, y))            # Point d'arriv√©e (right, y)

# =============================================================================
# BLOC 3: AGENT - Logique d'apprentissage par renforcement
# =============================================================================

class Agent:
    """Agent d'apprentissage par renforcement utilisant Deep Q-Learning."""

    def __init__(self):
        self.n_games = 0
        self.epsilon = 0  # Contr√¥le exploration vs exploitation
        self.gamma = GAMMA
        self.memory = deque(maxlen=MAX_MEMORY)  # M√©moire de replay

        # Mod√®le: 11 √©tats en entr√©e, 3 actions en sortie
        self.model = Linear_QNet(11, 256, 3)
        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)

        # Device (GPU/CPU)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

    def get_state(self, game):
        """
        Obtient l'√©tat actuel du jeu sous forme de vecteur de 11 √©l√©ments.

        √âtats (11 bits):
        - Danger [tout droit, droite, gauche] (3 bits)
        - Direction actuelle [gauche, droite, haut, bas] (4 bits)
        - Position nourriture [gauche, droite, haut, bas] (4 bits)
        """
        head = game.snake.head_pos

        # Points autour de la t√™te (pour d√©tecter les dangers)
        point_l = [head[0] - 1, head[1]]
        point_r = [head[0] + 1, head[1]]
        point_u = [head[0], head[1] - 1]
        point_d = [head[0], head[1] + 1]

        # Direction actuelle
        dir_l = game.snake.direction == LEFT
        dir_r = game.snake.direction == RIGHT
        dir_u = game.snake.direction == UP
        dir_d = game.snake.direction == DOWN

        state = [
            # Danger tout droit
            (dir_r and game.is_collision(point_r)) or
            (dir_l and game.is_collision(point_l)) or
            (dir_u and game.is_collision(point_u)) or
            (dir_d and game.is_collision(point_d)),

            # Danger √† droite
            (dir_u and game.is_collision(point_r)) or
            (dir_d and game.is_collision(point_l)) or
            (dir_l and game.is_collision(point_u)) or
            (dir_r and game.is_collision(point_d)),

            # Danger √† gauche
            (dir_d and game.is_collision(point_r)) or
            (dir_u and game.is_collision(point_l)) or
            (dir_r and game.is_collision(point_u)) or
            (dir_l and game.is_collision(point_d)),

            # Direction actuelle (4 bits one-hot)
            dir_l,
            dir_r,
            dir_u,
            dir_d,

            # Position de la nourriture par rapport √† la t√™te (4 bits)
            game.apple.position[0] < head[0],  # Pomme √† gauche
            game.apple.position[0] > head[0],  # Pomme √† droite
            game.apple.position[1] < head[1],  # Pomme en haut
            game.apple.position[1] > head[1]   # Pomme en bas
        ]

        return np.array(state, dtype=int)

    def remember(self, state, action, reward, next_state, done):
        """Stocke une exp√©rience dans la m√©moire de replay."""
        self.memory.append((state, action, reward, next_state, done))

    def train_long_memory(self):
        """Entra√Æne le mod√®le sur un batch d'exp√©riences pass√©es (replay d'exp√©rience)."""
        if len(self.memory) > BATCH_SIZE:
            mini_sample = random.sample(self.memory, BATCH_SIZE)
        else:
            mini_sample = self.memory

        states, actions, rewards, next_states, dones = zip(*mini_sample)
        self.trainer.train_step(states, actions, rewards, next_states, dones)

    def train_short_memory(self, state, action, reward, next_state, done):
        """Entra√Æne le mod√®le sur une seule exp√©rience (apprentissage imm√©diat)."""
        self.trainer.train_step(state, action, reward, next_state, done)

    def get_action(self, state):
        """
        Choisit une action bas√©e sur l'√©tat actuel.
        Utilise epsilon-greedy: exploration al√©atoire au d√©but, puis exploitation.

        Returns:
            action: [tout droit, droite, gauche] - un seul √©l√©ment √† 1
        """
        # Exploration vs Exploitation (epsilon diminue avec le temps)
        self.epsilon = EPSILON_START - self.n_games
        final_move = [0, 0, 0]

        if random.randint(0, 200) < self.epsilon:
            # Exploration: mouvement al√©atoire
            move = random.randint(0, 2)
            final_move[move] = 1
        else:
            # Exploitation: utilise le mod√®le pour pr√©dire la meilleure action
            state0 = torch.tensor(state, dtype=torch.float).to(self.device)
            prediction = self.model(state0)
            move = torch.argmax(prediction).item()
            final_move[move] = 1

        return final_move


class OptimizedAgent(Agent):
    """
    Version optimis√©e de l'Agent avec:
    - Batch processing am√©lior√© et doubl√© sur GPU
    - Utilisation de OptimizedQTrainer
    - Entra√Ænement long memory vectoris√©
    """
    def __init__(self):
        super().__init__()
        # Remplacer le trainer par la version optimis√©e
        self.trainer = OptimizedQTrainer(self.model, lr=LR, gamma=self.gamma)

        # Batch size plus grand pour mieux utiliser le GPU
        if torch.cuda.is_available():
            self.batch_size = BATCH_SIZE * 2
            print(f"üî• Batch Size GPU: {self.batch_size} (x2 pour optimisation)")
        else:
            self.batch_size = BATCH_SIZE
            print(f"üìä Batch Size CPU: {self.batch_size}")

    def train_long_memory_optimized(self):
        """
        Version optimis√©e avec batch processing parall√®le sur GPU.
        Traite tout le batch en une seule fois de mani√®re vectoris√©e.
        """
        if len(self.memory) < self.batch_size:
            # Pas assez de m√©moire, utiliser tout
            mini_sample = list(self.memory)
        else:
            # √âchantillonnage al√©atoire
            mini_sample = random.sample(self.memory, self.batch_size)

        # D√©compression des tuples (plus efficace que zip(*))
        states, actions, rewards, next_states, dones = map(list, zip(*mini_sample))

        # Entra√Ænement avec la version optimis√©e (vectoris√©e sur GPU)
        self.trainer.train_step_optimized(states, actions, rewards, next_states, dones)

    def get_action_batch(self, states):
        """
        Pr√©dit les actions pour plusieurs √©tats en parall√®le sur GPU.
        Utile pour traiter plusieurs exp√©riences simultan√©ment.

        Args:
            states: Liste d'√©tats numpy arrays

        Returns:
            Liste d'actions [tout droit, droite, gauche]
        """
        # Convertir en tensor batch
        states_tensor = torch.tensor(np.array(states), dtype=torch.float).to(self.device)

        # Pr√©diction parall√®le sur GPU (tr√®s rapide)
        with torch.no_grad():
            predictions = self.model(states_tensor)
            moves = torch.argmax(predictions, dim=1).cpu().numpy()

        # Convertir en format one-hot
        actions = []
        for move in moves:
            action = [0, 0, 0]
            action[move] = 1
            actions.append(action)

        return actions

# =============================================================================
# VISUALISATION
# =============================================================================

# Active le mode interactif de matplotlib
# Permet de mettre √† jour les graphiques en temps r√©el sans bloquer le programme
plt.ion()

def plot(scores, mean_scores):
    """
    Affiche les graphiques d'entra√Ænement en temps r√©el avec matplotlib.

    Cette fonction cr√©e un graphique dynamique montrant:
    - Les scores individuels de chaque partie (courbe bleue)
    - La moyenne mobile des scores (courbe rouge)

    Le graphique se met √† jour automatiquement apr√®s chaque partie termin√©e.

    Args:
        scores: Liste des scores de chaque partie [score1, score2, ...]
        mean_scores: Liste des scores moyens cumul√©s [moy1, moy2, ...]

    Technique utilis√©e:
        - IPython.display pour l'affichage dynamique
        - matplotlib.pyplot pour le trac√© des courbes
        - Mode interactif (ion) pour √©viter de bloquer le programme
    """
    # === √âTAPE 1: EFFACEMENT DE L'AFFICHAGE PR√âC√âDENT ===
    # display.clear_output() efface le graphique pr√©c√©dent dans la console/notebook
    # wait=True √©vite le scintillement en attendant que le nouveau graphique soit pr√™t
    # Cela permet une transition fluide entre les frames
    display.clear_output(wait=True)

    # === √âTAPE 2: AFFICHAGE DE LA FIGURE ACTUELLE ===
    # plt.gcf() = "get current figure" r√©cup√®re la figure matplotlib active
    # display.display() affiche cette figure dans la console IPython/Jupyter
    display.display(plt.gcf())

    # === √âTAPE 3: EFFACEMENT DU CONTENU DE LA FIGURE ===
    # plt.clf() = "clear figure" efface tout le contenu de la figure
    # Sans cela, les courbes se superposeraient √† chaque appel
    plt.clf()

    # === √âTAPE 4: CONFIGURATION DU GRAPHIQUE ===
    # D√©finit le titre principal du graphique (en haut)
    plt.title('Entra√Ænement Deep Q-Learning - Snake IA')
    # Label de l'axe X (horizontal) - repr√©sente le num√©ro de partie
    plt.xlabel('Nombre de parties')
    # Label de l'axe Y (vertical) - repr√©sente le score obtenu
    plt.ylabel('Score')

    # === √âTAPE 5: TRAC√â DES COURBES ===
    # Courbe des scores individuels (bleu transparent)
    # plt.plot(y_values, options...) trace une ligne reliant tous les points
    # alpha=0.6 rend la ligne semi-transparente (60% opaque)
    # Cela permet de voir la tendance sans surcharger visuellement
    plt.plot(scores, label='Score', color='blue', alpha=0.6)

    # Courbe du score moyen (rouge √©paisse)
    # linewidth=2 rend cette courbe plus visible (2 pixels d'√©paisseur)
    # C'est l'indicateur cl√©: montre si l'IA s'am√©liore dans le temps
    # Un score moyen croissant = apprentissage r√©ussi
    plt.plot(mean_scores, label='Score moyen', color='red', linewidth=2)

    # === √âTAPE 6: CONFIGURATION DES AXES ===
    # ylim(ymin=0) fixe le minimum de l'axe Y √† 0
    # Les scores ne peuvent pas √™tre n√©gatifs, donc on part de 0
    # ymax est automatique (s'adapte au score maximum)
    plt.ylim(ymin=0)

    # === √âTAPE 7: AJOUT DE LA L√âGENDE ===
    # Affiche une bo√Æte explicative identifiant chaque courbe
    # Utilise les 'label' d√©finis dans plt.plot()
    plt.legend()

    # === √âTAPE 8: AFFICHAGE DES VALEURS FINALES ===
    # plt.text(x, y, texte) place du texte √† une position donn√©e
    # len(scores)-1 = index de la derni√®re partie (position X)
    # scores[-1] = dernier score obtenu (position Y)
    # Affiche la valeur exacte du dernier score
    plt.text(len(scores)-1, scores[-1], str(scores[-1]))

    # Affiche la valeur du dernier score moyen (format√© avec 2 d√©cimales)
    # .2f = format float avec 2 chiffres apr√®s la virgule
    plt.text(len(mean_scores)-1, mean_scores[-1], f'{mean_scores[-1]:.2f}')

    # === √âTAPE 9: AJOUT DE LA GRILLE ===
    # plt.grid(True) active la grille de fond
    # alpha=0.3 rend la grille discr√®te (30% opaque)
    # Facilite la lecture des valeurs sans surcharger visuellement
    plt.grid(True, alpha=0.3)

    # === √âTAPE 10: AFFICHAGE ET RAFRA√éCHISSEMENT ===
    # plt.show(block=False) affiche le graphique sans bloquer le programme
    # block=False permet au code de continuer √† s'ex√©cuter
    # Si block=True, le programme attendrait que l'utilisateur ferme la fen√™tre
    plt.show(block=False)

    # === √âTAPE 11: PAUSE POUR RAFRA√éCHISSEMENT ===
    # plt.pause(0.1) fait une pause de 0.1 seconde (100 millisecondes)
    # Cette pause est N√âCESSAIRE pour que matplotlib actualise la fen√™tre
    # Sans cela, la fen√™tre pourrait ne pas se rafra√Æchir correctement
    plt.pause(.1)

# =============================================================================
# FONCTION D'ENTRA√éNEMENT ULTRA-RAPIDE (OPTIMIS√âE GPU)
# =============================================================================

def train_ultra_fast(max_games=10000):
    """
    Version ULTRA-RAPIDE avec toutes les optimisations GPU activ√©es:
    - Mixed precision training (float16) - gain ~30%
    - Batch processing doubl√©
    - Vectorisation maximale sur GPU
    - Entra√Ænement long memory tous les 5 jeux
    - Pas d'affichage pendant l'entra√Ænement

    Args:
        max_games: Nombre de parties (d√©faut: 10 000)

    Returns:
        record: Meilleur score
        scores: Liste des scores
    """
    print("=" * 60)
    print(f"üöÄ ENTRA√éNEMENT ULTRA-RAPIDE - {max_games} PARTIES")
    print("=" * 60)
    print(f"GPU disponible: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"üéÆ GPU: {gpu_name}")
        print(f"üíæ M√©moire GPU: {gpu_mem:.2f} GB")
        print(f"‚ö° Mixed Precision (float16): ACTIV√â")
        print(f"üî• Batch Size: {BATCH_SIZE * 2} (doubl√©)")
        print(f"‚ö° Gain de vitesse attendu: 10-15x")
    else:
        print(f"‚ö†Ô∏è  Mode CPU - Optimisations limit√©es")
    print("=" * 60)
    print("\n‚ö° Mode ultra-rapide - Affichage minimal")
    print("üìä Statistiques tous les 100 parties\n")

    scores = []
    total_score = 0
    record = 0

    # Utiliser l'agent optimis√© avec mixed precision
    agent = OptimizedAgent()

    # Jeu avec affichage (vitesse rapide mais visible)
    game = SnakeGameAI(speed=GAME_SPEED, display_enabled=True)

    # D√©sactiver matplotlib pendant l'entra√Ænement
    plt.ioff()

    while agent.n_games < max_games:
        # 1. √âtat actuel
        state_old = agent.get_state(game)

        # 2. Action
        final_move = agent.get_action(state_old)

        # 3. Jouer
        reward, done, score = game.play_step(final_move)
        state_new = agent.get_state(game)

        # 4. M√©moriser (pas d'entra√Ænement court pour aller plus vite)
        agent.remember(state_old, final_move, reward, state_new, done)

        if done:
            game.reset()
            agent.n_games += 1

            # Entra√Ænement long optimis√© tous les 5 jeux (batch processing GPU)
            if agent.n_games % 5 == 0:
                agent.train_long_memory_optimized()

            scores.append(score)
            total_score += score

            if score > record:
                record = score
                agent.model.save()
                print(f"üèÜ Partie {agent.n_games}/{max_games} | "
                      f"üéØ NOUVEAU RECORD: {record}")

            # Statistiques tous les 100 parties
            if agent.n_games % 100 == 0:
                mean_100 = sum(scores[-100:]) / min(100, len(scores))
                mean_total = total_score / agent.n_games
                print(f"üìä Partie {agent.n_games}/{max_games} | "
                      f"Score: {score} | "
                      f"Moy(100): {mean_100:.2f} | "
                      f"Moy(total): {mean_total:.2f} | "
                      f"Record: {record}")

    # Statistiques finales
    print("\n" + "=" * 60)
    print("üìà STATISTIQUES FINALES - ULTRA-RAPIDE")
    print("=" * 60)
    print(f"üèÜ Meilleur score: {record}")
    print(f"üìä Score moyen global: {total_score / len(scores):.2f}")
    print(f"üìä Score moyen (1000 derni√®res): {sum(scores[-1000:]) / 1000:.2f}")
    print(f"üìä Score moyen (100 derni√®res): {sum(scores[-100:]) / 100:.2f}")
    print(f"üíæ Mod√®le sauvegard√©: ./model/model.pth")
    print("=" * 60)

    return record, scores

# =============================================================================
# FONCTION D'ENTRA√éNEMENT RAPIDE (10 000 PARTIES)
# =============================================================================

def train_fast(max_games=10000):
    """
    Version rapide pour entra√Æner sur un grand nombre de parties.
    D√©sactive l'affichage graphique pour acc√©l√©rer l'entra√Ænement.

    Args:
        max_games: Nombre maximum de parties √† jouer (d√©faut: 10 000)

    Returns:
        record: Meilleur score atteint
        scores: Liste de tous les scores
    """
    print("=" * 60)
    print(f"üêç ENTRA√éNEMENT RAPIDE - {max_games} PARTIES üß†")
    print("=" * 60)
    print(f"GPU disponible: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU utilis√©: {torch.cuda.get_device_name(0)}")
        print(f"M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print("=" * 60)
    print("\n‚ö° Mode rapide activ√© - affichage minimal\n")

    scores = []
    total_score = 0
    record = 0
    agent = Agent()

    # Cr√©er le jeu avec affichage
    game = SnakeGameAI(speed=GAME_SPEED, display_enabled=True)

    # Masquer la fen√™tre pygame pour acc√©l√©rer (optionnel)
    # os.environ['SDL_VIDEODRIVER'] = 'dummy'

    while agent.n_games < max_games:
        # 1. Obtenir l'√©tat actuel
        state_old = agent.get_state(game)

        # 2. Obtenir l'action de l'agent
        final_move = agent.get_action(state_old)

        # 3. Jouer le mouvement
        reward, done, score = game.play_step(final_move)
        state_new = agent.get_state(game)

        # 4. Entra√Æner m√©moire courte
        agent.train_short_memory(state_old, final_move, reward, state_new, done)

        # 5. M√©moriser l'exp√©rience
        agent.remember(state_old, final_move, reward, state_new, done)

        if done:
            # 6. Entra√Æner m√©moire longue
            game.reset()
            agent.n_games += 1
            agent.train_long_memory()

            scores.append(score)
            total_score += score

            if score > record:
                record = score
                agent.model.save()
                print(f"üèÜ Partie {agent.n_games}/{max_games} | üéØ NOUVEAU RECORD: {record}")

            # Affichage tous les 100 parties
            if agent.n_games % 100 == 0:
                mean_100 = sum(scores[-100:]) / min(100, len(scores))
                mean_total = total_score / agent.n_games
                print(f"üìä Partie {agent.n_games}/{max_games} | "
                      f"Score: {score} | "
                      f"Moy(100): {mean_100:.2f} | "
                      f"Moy(total): {mean_total:.2f} | "
                      f"Record: {record}")

    # Statistiques finales
    print("\n" + "=" * 60)
    print("üìà STATISTIQUES FINALES - 10 000 PARTIES")
    print("=" * 60)
    print(f"üèÜ Meilleur score: {record}")
    print(f"üìä Score moyen global: {total_score / len(scores):.2f}")
    print(f"üìä Score moyen (1000 derni√®res): {sum(scores[-1000:]) / 1000:.2f}")
    print(f"üìä Score moyen (100 derni√®res): {sum(scores[-100:]) / 100:.2f}")
    print(f"üéÆ Parties jou√©es: {len(scores)}")
    print(f"üíæ Mod√®le sauvegard√©: ./model/model.pth")
    print("=" * 60)

    return record, scores

# =============================================================================
# FONCTION D'ENTRA√éNEMENT PRINCIPALE
# =============================================================================

def train(display_game=False):
    """
    Fonction principale d'entra√Ænement de l'agent.

    Args:
        display_game: Si True, affiche la fen√™tre du jeu (plus lent)
                     Si False, entra√Ænement sans affichage (rapide)
    """
    print("=" * 60)
    print("üêç SNAKE IA - DEEP Q-LEARNING üß†")
    print("=" * 60)
    print(f"GPU disponible: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU utilis√©: {torch.cuda.get_device_name(0)}")
        print(f"M√©moire GPU disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print("=" * 60)
    if display_game:
        print("\nüéÆ Mode avec affichage du jeu (plus lent)")
    else:
        print("\n‚ö° Mode sans affichage du jeu (rapide)")
    print("\nD√©marrage de l'entra√Ænement...\n")

    plot_scores = []
    plot_mean_scores = []
    total_score = 0
    record = 0
    agent = Agent()
    game = SnakeGameAI(display_enabled=display_game)

    while True:
        # 1. Obtenir l'√©tat actuel
        state_old = agent.get_state(game)

        # 2. Obtenir le mouvement de l'agent (model.predict)
        final_move = agent.get_action(state_old)

        # 3. Effectuer le mouvement et obtenir les r√©sultats
        reward, done, score = game.play_step(final_move)
        state_new = agent.get_state(game)

        # 4. Entra√Æner la m√©moire courte (model.train - imm√©diat)
        agent.train_short_memory(state_old, final_move, reward, state_new, done)

        # 5. Remember (stocker l'exp√©rience)
        agent.remember(state_old, final_move, reward, state_new, done)

        if done:
            # 6. Entra√Æner la m√©moire longue (model.train - replay)
            game.reset()
            agent.n_games += 1
            agent.train_long_memory()

            if score > record:
                record = score
                agent.model.save()
                print(f"üèÜ Nouveau record! Score: {record} | Partie: {agent.n_games}")

            print(f'Partie {agent.n_games} | Score: {score} | Record: {record}')

            # Mettre √† jour les graphiques
            plot_scores.append(score)
            total_score += score
            mean_score = total_score / agent.n_games
            plot_mean_scores.append(mean_score)
            plot(plot_scores, plot_mean_scores)

# =============================================================================
# POINT D'ENTR√âE
# =============================================================================

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Snake IA - Deep Q-Learning Optimis√©')
    parser.add_argument('--ultra', action='store_true',
                       help='üöÄ Mode ULTRA-RAPIDE avec toutes les optimisations GPU (recommand√©)')
    parser.add_argument('--fast', action='store_true',
                       help='‚ö° Mode rapide standard pour 10 000 parties')
    parser.add_argument('--games', type=int, default=10000,
                       help='Nombre de parties (d√©faut: 10000)')
    parser.add_argument('--display', action='store_true',
                       help='üéÆ Afficher la fen√™tre du jeu (mode normal uniquement)')

    args = parser.parse_args()

    if args.ultra:
        print("üöÄ Mode ULTRA-RAPIDE s√©lectionn√© (Optimisations GPU)\n")
        record, scores = train_ultra_fast(max_games=args.games)
    elif args.fast:
        print("‚ö° Mode rapide s√©lectionn√©\n")
        record, scores = train_fast(max_games=args.games)
    else:
        if args.display:
            print("üéÆ Mode normal avec affichage du jeu\n")
            train(display_game=True)
        else:
            print("üìä Mode normal sans affichage du jeu (graphiques uniquement)\n")
            train(display_game=False)


# -*- coding: utf-8 -*-
"""
Snake IA - Deep Q-Learning
Groupe 20 - SIGL
Apprentissage par renforcement pour le jeu du serpent
Utilise serpent.py comme base du jeu
"""
import pygame
import random
import numpy as np
from collections import deque
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from matplotlib.backends.backend_agg import FigureCanvasAgg
import time
import sys
import os

# Ajouter le r√©pertoire parent au path pour importer serpent.py
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import serpent

# Configuration de l'encodage pour Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# =============================================================================
# CONSTANTES DU JEU (import√©es de serpent.py)
# =============================================================================

# Utilisation des constantes de serpent.py
GRID_SIZE = serpent.GRID_SIZE
CELL_SIZE = serpent.CELL_SIZE
GAME_SPEED = 40  # FPS pour l'entra√Ænement rapide (plus rapide que le jeu normal)

SCREEN_WIDTH = serpent.SCREEN_WIDTH
SCORE_PANEL_HEIGHT = 120  # Panneau plus grand pour les infos d'entra√Ænement
SCREEN_HEIGHT = SCREEN_WIDTH + SCORE_PANEL_HEIGHT

# Couleurs (de serpent.py)
BLANC = serpent.BLANC
NOIR = serpent.NOIR
ORANGE = serpent.ORANGE
VERT = serpent.VERT
ROUGE = serpent.ROUGE
GRIS_FOND = serpent.GRIS_FOND
GRIS_GRILLE = serpent.GRIS_GRILLE
BLEU = (0, 100, 255)
JAUNE = (255, 255, 0)

# Directions (de serpent.py)
UP = serpent.UP
DOWN = serpent.DOWN
LEFT = serpent.LEFT
RIGHT = serpent.RIGHT

# Actions relatives (pour l'IA)
STRAIGHT = 0
RIGHT_TURN = 1
LEFT_TURN = 2

# =============================================================================
# MOD√àLE DQN (DEEP Q-NETWORK)
# =============================================================================

class DQN(nn.Module):
    """
    R√©seau de neurones SIMPLIFI√â pour apprentissage plus rapide.

    Architecture SIMPLIFI√âE:
    - Input: 11 features (vecteur d'√©tat)
    - Hidden: 128 neurones (ReLU) - PLUS SIMPLE !
    - Output: 3 neurones (Q-values pour chaque action)
    """

    def __init__(self, input_size=11, hidden_size=128, output_size=3):
        super(DQN, self).__init__()

        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# =============================================================================
# AGENT D'APPRENTISSAGE PAR RENFORCEMENT
# =============================================================================

class AgentDQN:
    """
    Agent qui apprend √† jouer au Snake via Deep Q-Learning.

    Caract√©ristiques:
    - M√©moire de replay pour briser les corr√©lations temporelles
    - Strat√©gie epsilon-greedy pour l'exploration vs exploitation
    - Entra√Ænement par batch avec l'√©quation de Bellman
    """

    def __init__(self, learning_rate=0.001, gamma=0.9):
        self.n_games = 0
        self.epsilon = 0  # Sera g√©r√© par le syst√®me d'entra√Ænement
        self.gamma = gamma  # Facteur de discount
        self.memory = deque(maxlen=100_000)  # M√©moire de replay

        self.model = DQN()
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

    def remember(self, state, action, reward, next_state, done):
        """Stocke une transition dans la m√©moire de replay."""
        self.memory.append((state, action, reward, next_state, done))

    def get_action(self, state, epsilon=0):
        """
        Choisit une action via epsilon-greedy:
        - avec probabilit√© epsilon: action al√©atoire (exploration)
        - sinon: action avec la meilleure Q-value (exploitation)
        """
        if random.random() < epsilon:
            return random.randint(0, 2)  # Action al√©atoire

        # Pr√©diction du mod√®le
        state_tensor = torch.tensor(state, dtype=torch.float, device=self.device)
        self.model.eval()
        with torch.no_grad():
            q_values = self.model(state_tensor)
        self.model.train()

        return torch.argmax(q_values).item()

    def train_step(self, state, action, reward, next_state, done):
        """
        Entra√Æne le mod√®le sur une seule transition.
        Utilise l'√©quation de Bellman: Q(s,a) = r + Œ≥ √ó max(Q(s',a'))
        """
        state = torch.tensor(state, dtype=torch.float, device=self.device)
        next_state = torch.tensor(next_state, dtype=torch.float, device=self.device)
        action = torch.tensor(action, dtype=torch.long, device=self.device)
        reward = torch.tensor(reward, dtype=torch.float, device=self.device)

        if len(state.shape) == 1:
            state = torch.unsqueeze(state, 0)
            next_state = torch.unsqueeze(next_state, 0)
            action = torch.unsqueeze(action, 0)
            reward = torch.unsqueeze(reward, 0)
            done = (done, )

        # 1. Q-values pr√©dites pour l'√©tat actuel
        pred = self.model(state)

        target = pred.clone()
        for idx in range(len(done)):
            Q_new = reward[idx]
            if not done[idx]:
                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))

            target[idx][action[idx].item()] = Q_new

        # 2. Calcul de la perte et backpropagation
        self.optimizer.zero_grad()
        loss = self.criterion(target, pred)
        loss.backward()
        self.optimizer.step()

    def train_batch(self, batch_size=1000):
        """
        Entra√Æne le mod√®le sur un batch al√©atoire de la m√©moire.
        Cette m√©thode permet de briser les corr√©lations temporelles.
        """
        if len(self.memory) < batch_size:
            return

        # √âchantillonnage al√©atoire
        batch = random.sample(self.memory, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        self.train_step(states, actions, rewards, next_states, dones)

    def save_model(self, filename="model.pth"):
        """Sauvegarde le mod√®le."""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'n_games': self.n_games
        }, filename)
        print(f"Mod√®le sauvegard√©: {filename}")

    def load_model(self, filename="model.pth"):
        """Charge un mod√®le pr√©-entra√Æn√©."""
        checkpoint = torch.load(filename)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.n_games = checkpoint['n_games']
        print(f"Mod√®le charg√©: {filename}")

# =============================================================================
# JEU DE SNAKE ADAPT√â POUR L'IA (utilise serpent.py)
# =============================================================================

class SnakeGameIA:
    """
    Wrapper autour des classes Snake et Apple de serpent.py pour l'IA.

    Adaptation pour l'apprentissage par renforcement:
    - Utilise les classes Snake et Apple de serpent.py
    - Interface simplifi√©e pour l'agent IA
    - Syst√®me de r√©compenses int√©gr√©
    - Fonction get_state() qui retourne le vecteur d'√©tat (11 dimensions)
    - M√©thode play_step() qui ex√©cute une action et retourne la r√©compense
    """

    def __init__(self, render=True):
        self.render_mode = render

        if self.render_mode:
            pygame.init()
            self.screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))
            pygame.display.set_caption("Snake IA - Deep Q-Learning (serpent.py)")
            self.clock = pygame.time.Clock()
            self.font = pygame.font.Font(None, 30)
            self.font_small = pygame.font.Font(None, 20)

        self.reset()

    def reset(self):
        """R√©initialise le jeu pour une nouvelle partie."""
        # Utilise les classes de serpent.py
        self.snake = serpent.Snake()
        self.apple = serpent.Apple(self.snake.body)
        self.frame_iteration = 0
        self.start_time = time.time()
        self.prev_distance = self._get_distance_to_food()  # Pour reward shaping

        return self.get_state()

    def _get_distance_to_food(self):
        """Calcule la distance de Manhattan entre la t√™te et la pomme."""
        if not self.food_pos:
            return 0
        return abs(self.head_pos[0] - self.food_pos[0]) + abs(self.head_pos[1] - self.food_pos[1])

    @property
    def head_pos(self):
        """Raccourci pour acc√©der √† la position de la t√™te."""
        return self.snake.head_pos

    @property
    def body(self):
        """Raccourci pour acc√©der au corps du serpent."""
        return self.snake.body

    @property
    def direction(self):
        """Raccourci pour acc√©der √† la direction."""
        return self.snake.direction

    @property
    def score(self):
        """Raccourci pour acc√©der au score."""
        return self.snake.score

    @property
    def food_pos(self):
        """Raccourci pour acc√©der √† la position de la pomme."""
        return list(self.apple.position) if self.apple.position else None

    def get_state(self):
        """
        Retourne le vecteur d'√©tat (11 dimensions) repr√©sentant la situation actuelle.

        Structure du vecteur:
        [0-2]: Dangers imm√©diats (devant, droite, gauche)
        [3-6]: Direction actuelle (one-hot: gauche, droite, haut, bas)
        [7-10]: Position relative de la pomme (gauche, droite, haut, bas)
        """
        head = self.head_pos

        # Points dans les 3 directions possibles
        point_l = self._get_point_in_direction(self._turn_left(self.direction))
        point_r = self._get_point_in_direction(self._turn_right(self.direction))
        point_f = self._get_point_in_direction(self.direction)

        # Direction actuelle (one-hot)
        dir_l = self.direction == LEFT
        dir_r = self.direction == RIGHT
        dir_u = self.direction == UP
        dir_d = self.direction == DOWN

        # Position de la pomme (gestion du cas o√π food_pos est None)
        food_pos = self.food_pos if self.food_pos else [0, 0]

        state = [
            # Danger devant, droite, gauche
            self._is_collision(point_f),
            self._is_collision(point_r),
            self._is_collision(point_l),

            # Direction actuelle
            dir_l,
            dir_r,
            dir_u,
            dir_d,

            # Position de la pomme
            food_pos[0] < head[0],  # Pomme √† gauche
            food_pos[0] > head[0],  # Pomme √† droite
            food_pos[1] < head[1],  # Pomme en haut
            food_pos[1] > head[1]   # Pomme en bas
        ]

        return np.array(state, dtype=int)

    def _get_point_in_direction(self, direction):
        """Retourne le point adjacent dans une direction donn√©e."""
        x = (self.head_pos[0] + direction[0]) % GRID_SIZE
        y = (self.head_pos[1] + direction[1]) % GRID_SIZE
        return [x, y]

    def _turn_left(self, direction):
        """Retourne la direction apr√®s un virage √† gauche."""
        if direction == UP: return LEFT
        if direction == LEFT: return DOWN
        if direction == DOWN: return RIGHT
        if direction == RIGHT: return UP

    def _turn_right(self, direction):
        """Retourne la direction apr√®s un virage √† droite."""
        if direction == UP: return RIGHT
        if direction == RIGHT: return DOWN
        if direction == DOWN: return LEFT
        if direction == LEFT: return UP

    def _is_collision(self, point):
        """V√©rifie si un point est une collision (mur ou corps)."""
        # Note: les murs n'existent plus avec le wrapping, mais on garde la structure
        # pour compatibilit√© si on veut ajouter les murs plus tard
        return point in self.body

    def play_step(self, action):
        """
        Ex√©cute une action et retourne (reward, game_over, score).

        Actions:
        - 0: Tout droit
        - 1: Virage √† droite
        - 2: Virage √† gauche

        Syst√®me de r√©compenses ULTRA SIMPLIFI√â (apprentissage garanti):
        - Manger pomme: +100 (√©norme r√©compense !)
        - Se rapprocher: +10 (grosse r√©compense)
        - S'√©loigner: -15 (grosse p√©nalit√©)
        - Collision: -100 (tr√®s mauvais)
        - Victoire: +1000
        """
        self.frame_iteration += 1

        # 1. Gestion des √©v√©nements
        if self.render_mode:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    quit()

        # 2. Mise √† jour de la direction selon l'action (utilise set_direction de serpent.py)
        new_direction = self.direction
        if action == RIGHT_TURN:
            new_direction = self._turn_right(self.direction)
        elif action == LEFT_TURN:
            new_direction = self._turn_left(self.direction)
        # Sinon: STRAIGHT, on garde la direction actuelle

        self.snake.set_direction(new_direction)

        # 3. D√©placement du serpent (utilise la m√©thode move() de serpent.py)
        self.snake.move()

        # 4. Calcul de la r√©compense avec REWARD SHAPING ULTRA SIMPLIFI√â
        reward = 0
        game_over = False

        # V√©rification de collision (utilise check_self_collision de serpent.py)
        # Note: serpent.py utilise le wrapping, donc pas de collision avec les murs
        if self.snake.check_self_collision() or self.frame_iteration > 100 * len(self.body):
            game_over = True
            reward = -100  # GROSSE p√©nalit√© pour mourir
            if self.render_mode:
                self.render()
            return reward, game_over, self.score

        # V√©rification de la pomme mang√©e
        if self.head_pos == list(self.apple.position):
            self.snake.grow()  # Utilise la m√©thode grow() de serpent.py
            reward = 100  # GROSSE r√©compense pour manger !

            # Tente de replacer la pomme (utilise relocate() de serpent.py)
            if not self.apple.relocate(self.snake.body):
                # Victoire compl√®te (toutes les cases sont remplies)
                reward = 1000  # √âNORME r√©compense !
                game_over = True

            # Mise √† jour de la distance pr√©c√©dente
            self.prev_distance = self._get_distance_to_food()
        else:
            # REWARD SHAPING SIMPLIFI√â: r√©compenses BEAUCOUP PLUS FORTES
            current_distance = self._get_distance_to_food()

            if current_distance < self.prev_distance:
                # Se rapproche de la pomme : GROSSE R√âCOMPENSE
                reward = 10
            else:
                # S'√©loigne de la pomme : GROSSE P√âNALIT√â
                reward = -15

            self.prev_distance = current_distance

        # 5. Rendu visuel
        if self.render_mode:
            self.render()
            self.clock.tick(GAME_SPEED)

        return reward, game_over, self.score

    def render(self):
        """Affiche l'√©tat actuel du jeu en utilisant les m√©thodes de serpent.py."""
        if not self.render_mode:
            return

        # Fond
        self.screen.fill(GRIS_FOND)
        game_area_rect = pygame.Rect(0, SCORE_PANEL_HEIGHT, SCREEN_WIDTH, SCREEN_WIDTH)
        pygame.draw.rect(self.screen, NOIR, game_area_rect)

        # Grille (utilise la fonction de serpent.py adapt√©e)
        self._draw_grid()

        # Dessine la pomme et le serpent en utilisant leurs m√©thodes draw() de serpent.py
        self.apple.draw(self.screen)
        self.snake.draw(self.screen)

        # Informations du panneau sup√©rieur
        pygame.draw.line(self.screen, BLANC,
                        (0, SCORE_PANEL_HEIGHT - 2),
                        (SCREEN_WIDTH, SCORE_PANEL_HEIGHT - 2), 2)

        score_text = self.font.render(f"Score: {self.score}", True, BLANC)
        self.screen.blit(score_text, (10, 20))

        max_cells = GRID_SIZE * GRID_SIZE
        fill_rate = (len(self.body) / max_cells) * 100
        fill_text = self.font_small.render(f"Remplissage: {fill_rate:.1f}%", True, BLANC)
        self.screen.blit(fill_text, (10, 55))

        frames_text = self.font_small.render(f"Frames: {self.frame_iteration}", True, BLANC)
        self.screen.blit(frames_text, (10, 85))

        pygame.display.flip()

    def _draw_grid(self):
        """Dessine la grille pour une meilleure visualisation."""
        for x in range(0, SCREEN_WIDTH, CELL_SIZE):
            pygame.draw.line(self.screen, GRIS_GRILLE,
                           (x, SCORE_PANEL_HEIGHT), (x, SCREEN_HEIGHT))
        for y in range(SCORE_PANEL_HEIGHT, SCREEN_HEIGHT, CELL_SIZE):
            pygame.draw.line(self.screen, GRIS_GRILLE, (0, y), (SCREEN_WIDTH, y))

# =============================================================================
# SYST√àME D'ENTRA√éNEMENT AVEC M√âTRIQUES
# =============================================================================

class TrainingMetrics:
    """
    G√®re les m√©triques de performance pendant l'entra√Ænement.
    - Scores par partie
    - Temps de survie
    - Moyennes mobiles
    """

    def __init__(self):
        self.scores = []
        self.mean_scores = []
        self.total_score = 0
        self.record = 0

        # Pour la visualisation en temps r√©el
        plt.ion()
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1, figsize=(10, 8))

    def update(self, score):
        """Met √† jour les m√©triques apr√®s une partie."""
        self.scores.append(score)
        self.total_score += score
        mean_score = self.total_score / len(self.scores)
        self.mean_scores.append(mean_score)

        if score > self.record:
            self.record = score

    def plot(self, epsilon=0):
        """Affiche les courbes d'apprentissage."""
        self.ax1.clear()
        self.ax2.clear()

        # Graphique 1: Scores
        self.ax1.set_title('Courbe d\'apprentissage - Deep Q-Learning')
        self.ax1.set_xlabel('Nombre de parties')
        self.ax1.set_ylabel('Score')
        self.ax1.plot(self.scores, label='Score', alpha=0.6)
        self.ax1.plot(self.mean_scores, label='Score moyen', linewidth=2)
        self.ax1.axhline(y=self.record, color='r', linestyle='--',
                        label=f'Record: {self.record}')
        self.ax1.legend()
        self.ax1.grid(True, alpha=0.3)

        # Graphique 2: Epsilon (exploration)
        n_games = len(self.scores)
        self.ax2.set_title('Taux d\'exploration (Epsilon)')
        self.ax2.set_xlabel('Nombre de parties')
        self.ax2.set_ylabel('Epsilon')
        self.ax2.axhline(y=epsilon, color='g', linewidth=2)
        self.ax2.set_ylim(0, 1)
        self.ax2.grid(True, alpha=0.3)
        self.ax2.text(0.5, 0.5, f'Œµ = {epsilon:.3f}',
                     transform=self.ax2.transAxes,
                     fontsize=40, ha='center', va='center',
                     color='green', alpha=0.3)

        plt.tight_layout()
        plt.pause(0.001)

    def save_plot(self, filename='training_progress.png'):
        """Sauvegarde le graphique."""
        self.fig.savefig(filename)
        print(f"Graphique sauvegard√©: {filename}")

# =============================================================================
# BOUCLE D'ENTRA√éNEMENT PRINCIPALE
# =============================================================================

def train(render=True, max_games=500):
    """
    Boucle d'entra√Ænement ULTRA SIMPLIFI√âE - APPRENTISSAGE GARANTI !

    Param√®tres:
    - render: Afficher le jeu pendant l'entra√Ænement
    - max_games: Nombre maximum de parties d'entra√Ænement

    Strat√©gie ULTRA SIMPLE:
    - Epsilon initial: 30% seulement (moins d'al√©atoire)
    - Epsilon minimum: 0% (exploitation pure √† la fin)
    - D√©croissance: rapide sur 50% des parties
    - Learning rate TR√àS √©lev√©: 0.005 (apprentissage ultra rapide)
    - R√©compenses √âNORMES pour guider l'IA
    """
    print("="*70)
    print(" ENTRAINEMENT ULTRA SIMPLIFI√â - APPRENTISSAGE GARANTI ")
    print("="*70)
    print(f"Nombre de parties: {max_games}")
    print(f"Render: {'Activ√©' if render else 'D√©sactiv√©'}")
    print(f"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}")
    print("\nSTRAT√âGIE ULTRA SIMPLE:")
    print("  ‚Ä¢ R√©compenses √âNORMES:")
    print("    - Manger pomme: +100 (au lieu de +10)")
    print("    - Se rapprocher: +10 (au lieu de +1)")
    print("    - S'√©loigner: -15 (au lieu de -1.5)")
    print("    - Mourir: -100 (au lieu de -10)")
    print("  ‚Ä¢ R√©seau simplifi√©: 1 couche de 128 neurones")
    print("  ‚Ä¢ Epsilon: 30% ‚Üí 0% (moins d'al√©atoire)")
    print("  ‚Ä¢ Learning rate: 0.005 (apprentissage ULTRA rapide)")
    print("="*70 + "\n")

    # Initialisation avec param√®tres ULTRA SIMPLIFI√âS
    agent = AgentDQN(learning_rate=0.005, gamma=0.99)  # LR tr√®s √©lev√©, gamma maximal

    # CHARGEMENT AUTOMATIQUE du mod√®le existant s'il existe
    model_path = "model_final.pth"
    if os.path.exists(model_path):
        print(f"‚úì Mod√®le existant d√©tect√© : {model_path}")
        print("  Chargement de l'apprentissage pr√©c√©dent...")
        agent.load_model(model_path)
        print(f"  L'IA a d√©j√† jou√© {agent.n_games} parties !")
        print("  L'entra√Ænement va CONTINUER √† partir de l√†.\n")
    else:
        print("‚úó Aucun mod√®le existant trouv√©.")
        print("  D√©marrage d'un NOUVEL entra√Ænement.\n")

    game = SnakeGameIA(render=render)
    metrics = TrainingMetrics()

    # Param√®tres epsilon-greedy ULTRA SIMPLIFI√âS
    epsilon_start = 0.3   # Seulement 30% d'al√©atoire au d√©but
    epsilon_end = 0.0     # 0% √† la fin (exploitation pure)
    epsilon_decay = (epsilon_start - epsilon_end) / (max_games * 0.5)  # D√©croissance rapide
    epsilon = epsilon_start

    # Boucle d'entra√Ænement
    for n_game in range(max_games):
        # Reset du jeu
        state = game.reset()
        game_over = False

        # Une partie compl√®te
        while not game_over:
            # 1. Choisir une action (epsilon-greedy)
            action = agent.get_action(state, epsilon)

            # 2. Ex√©cuter l'action
            reward, game_over, score = game.play_step(action)

            # 3. Observer le nouvel √©tat
            next_state = game.get_state()

            # 4. M√©moriser l'exp√©rience
            agent.remember(state, action, reward, next_state, game_over)

            # 5. Entra√Æner sur cette transition
            agent.train_step(state, action, reward, next_state, game_over)

            state = next_state

        # Apr√®s chaque partie
        agent.n_games += 1

        # Entra√Ænement par batch si assez de m√©moire
        if len(agent.memory) > 1000:
            agent.train_batch(batch_size=1000)

        # Mise √† jour des m√©triques
        metrics.update(score)

        # D√©croissance d'epsilon
        if epsilon > epsilon_end:
            epsilon = max(epsilon_end, epsilon - epsilon_decay)

        # Affichage et sauvegarde p√©riodiques
        if (n_game + 1) % 10 == 0:
            avg_score = metrics.mean_scores[-1]
            progress_percent = (n_game + 1) / max_games * 100
            print(f"[{progress_percent:5.1f}%] Partie {n_game + 1}/{max_games} | "
                  f"Score: {score:2d} | "
                  f"Record: {metrics.record:2d} | "
                  f"Moy: {avg_score:5.2f} | "
                  f"Œµ: {epsilon:.3f}")

            # Affichage d'un message d'encouragement si le record augmente
            if score == metrics.record and score > 0:
                print(f"    üéØ NOUVEAU RECORD ! Le serpent a mang√© {score} pomme(s) !")

            metrics.plot(epsilon)

        # Sauvegarde du mod√®le tous les 50 parties
        if (n_game + 1) % 50 == 0:
            agent.save_model(f"model_game_{n_game+1}.pth")

    # Fin de l'entra√Ænement
    print("\n" + "="*70)
    print(" ENTRAINEMENT TERMINE ")
    print("="*70)
    print(f"Record: {metrics.record}")
    print(f"Score moyen final: {metrics.mean_scores[-1]:.2f}")
    print(f"Nombre total de parties: {agent.n_games}")

    # Sauvegarde finale
    agent.save_model("model_final.pth")
    metrics.save_plot("training_progress.png")

    plt.ioff()
    plt.show()

# =============================================================================
# MODE JEU (TEST DU MOD√àLE ENTRA√éN√â)
# =============================================================================

def play_with_model(model_path="model_final.pth"):
    """
    Fait jouer l'agent avec un mod√®le pr√©-entra√Æn√©.
    Mode d√©monstration sans exploration (epsilon=0).
    """
    print("="*70)
    print(" MODE JEU - AGENT ENTRAINE ")
    print("="*70)

    agent = AgentDQN()
    agent.load_model(model_path)
    game = SnakeGameIA(render=True)

    total_score = 0
    n_games = 0

    print("Appuyez sur ESC pour quitter\n")

    while True:
        state = game.reset()
        game_over = False

        while not game_over:
            # Action sans exploration (epsilon=0)
            action = agent.get_action(state, epsilon=0)
            reward, game_over, score = game.play_step(action)
            state = game.get_state()

            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    return
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        pygame.quit()
                        return

        n_games += 1
        total_score += score
        print(f"Partie {n_games} | Score: {score} | Moyenne: {total_score/n_games:.2f}")

        time.sleep(1)  # Pause entre les parties

# =============================================================================
# POINT D'ENTR√âE
# =============================================================================

if __name__ == '__main__':
    import sys

    print("\n" + "="*70)
    print(" SNAKE AI - DEEP Q-LEARNING ")
    print(" Groupe 20 - SIGL ")
    print("="*70 + "\n")

    if len(sys.argv) > 1 and sys.argv[1] == "play":
        # Mode jeu avec mod√®le pr√©-entra√Æn√©
        play_with_model()
    else:
        # Mode entra√Ænement
        print("Options:")
        print("1. Entra√Ænement avec rendu visuel (lent mais instructif)")
        print("2. Entra√Ænement sans rendu (rapide)")
        print("3. Jouer avec un mod√®le pr√©-entra√Æn√©")

        choice = input("\nVotre choix (1/2/3): ").strip()

        if choice == "1":
            train(render=True, max_games=500)
        elif choice == "2":
            train(render=False, max_games=1000)
        elif choice == "3":
            play_with_model()
        else:
            print("Choix invalide. Lancement de l'entra√Ænement par d√©faut...")
            train(render=True, max_games=500)
